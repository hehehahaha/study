LR和SVM的区别

一、相同点
　第一，LR和SVM都是分类算法（SVM也可以用与回归）
　第二，如果不考虑核函数，LR和SVM都是线性分类算法，也就是说他们的分类决策面都是线性的。
　　这里要先说明一点，那就是LR也是可以用核函数的。总之，原始的LR和SVM都是线性分类器，这也是为什么通常没人问你决策树和LR什么区别，你说一个非线性分类器和一个线性分类器有什么区别？
　第三，LR和SVM都是监督学习算法。
　第四，LR和SVM都是判别模型。


二、不同点

　第一，本质上是其loss function不同
　第二，支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用，虽然作用会相对小一些）
  第三，在解决非线性问题时，支持向量机采用核函数的机制，而LR通常不采用核函数的方法
  第四，线性SVM依赖数据表达的距离测度，所以需要对数据先做normalization，LR不受其影响
  第五，SVM的损失函数就自带正则！！！（损失函数中的1/2||w||^2项），这就是为什么SVM是结构风险最小化算法的原因！！！而LR必须另外在损失函数上添加正则项！！！

